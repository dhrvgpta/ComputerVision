{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversational Language Models: RAG, Fine-tuning, and MCP Server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part A: Retrieval Augmented Generation (RAG) Pipeline [10 points]\n",
        "\n",
        "This section implements a RAG pipeline using a pre-trained conversational language model from Hugging Face. The RAG system enhances model responses by retrieving relevant information from an external knowledge base.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: 0.26.0 not found\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers langchain_community sentence-transformers datasets peft faiss-cpu accelerate>=0.26.0 google.generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/dhruv/Documents/Projects/MachineLearning/Computer Vision/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model: microsoft/DialoGPT-medium\n",
            "Model parameters: 354,823,168\n",
            "Model moved to device: mps\n"
          ]
        }
      ],
      "source": [
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = model.to(device)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Loaded model: {model_name}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Model moved to device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Knowledge base created with AI/ML domain information\n"
          ]
        }
      ],
      "source": [
        "knowledge_base_text = \"\"\"\n",
        "Machine Learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. \n",
        "Deep Learning uses neural networks with multiple layers to model and understand complex patterns in data.\n",
        "Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language.\n",
        "Transformers are a type of neural network architecture that has revolutionized NLP, introduced in the paper \"Attention is All You Need\".\n",
        "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model that reads text bidirectionally.\n",
        "GPT (Generative Pre-trained Transformer) is a family of autoregressive language models that generate text sequentially.\n",
        "Retrieval Augmented Generation (RAG) combines information retrieval with text generation to provide more accurate and contextual responses.\n",
        "Vector databases store embeddings of text documents to enable semantic search and retrieval.\n",
        "Fine-tuning adapts pre-trained models to specific tasks or domains by training on task-specific data.\n",
        "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"knowledge_base.txt\", \"w\") as f:\n",
        "    f.write(knowledge_base_text)\n",
        "\n",
        "print(\"Knowledge base created with AI/ML domain information\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created vector store with 9 document chunks\n"
          ]
        }
      ],
      "source": [
        "loader = TextLoader(\"knowledge_base.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(documents=texts, embedding=embeddings)\n",
        "vectorstore.save_local(\"./faiss_index\")\n",
        "print(f\"Created vector store with {len(texts)} document chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG pipeline initialized\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=150,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "def rag_query(question):\n",
        "    docs = retriever.invoke(question)\n",
        "    context = \" \".join([doc.page_content for doc in docs])\n",
        "    \n",
        "    prompt_text = f\"\"\"Use the following context to answer the question. \n",
        "If you don't know the answer, just say that you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    result = llm.invoke(prompt_text)\n",
        "    return {\"answer\": result, \"context\": docs}\n",
        "\n",
        "print(\"RAG pipeline initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What is Retrieval Augmented Generation?\n",
            "\n",
            "Answer: Use the following context to answer the question. \n",
            "If you don't know the answer, just say that you don't know.\n",
            "\n",
            "Context: Retrieval Augmented Generation (RAG) combines information retrieval with text generation to provide more accurate and contextual responses. GPT (Generative Pre-trained Transformer) is a family of autoregressive language models that generate text sequentially. Vector databases store embeddings of text documents to enable semantic search and retrieval.\n",
            "Fine-tuning adapts pre-trained models to specific tasks or domains by training on task-specific data.\n",
            "\n",
            "Question: What is Retrieval Augmented Generation?\n",
            "\n",
            "Answer:\n",
            "\n",
            "Retrieved Documents:\n",
            "\n",
            "Document 1: Retrieval Augmented Generation (RAG) combines information retrieval with text generation to provide ...\n",
            "\n",
            "Document 2: GPT (Generative Pre-trained Transformer) is a family of autoregressive language models that generate...\n"
          ]
        }
      ],
      "source": [
        "query = \"What is Retrieval Augmented Generation?\"\n",
        "result = rag_query(query)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nAnswer:\", result[\"answer\"])\n",
        "print(\"\\nRetrieved Documents:\")\n",
        "for i, doc in enumerate(result[\"context\"][:2], 1):\n",
        "    print(f\"\\nDocument {i}: {doc.page_content[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part B: Fine-tuning with Domain-Specific Dataset\n",
        "\n",
        "This section demonstrates fine-tuning a pre-trained conversational language model on a domain-specific dataset. We use LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model for fine-tuning: gpt2\n",
            "Model moved to device: mps\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "fine_tune_model_name = \"gpt2\"\n",
        "fine_tune_tokenizer = AutoTokenizer.from_pretrained(fine_tune_model_name)\n",
        "fine_tune_model = AutoModelForCausalLM.from_pretrained(fine_tune_model_name)\n",
        "fine_tune_model = fine_tune_model.to(device)\n",
        "\n",
        "fine_tune_tokenizer.pad_token = fine_tune_tokenizer.eos_token\n",
        "\n",
        "print(f\"Loaded model for fine-tuning: {fine_tune_model_name}\")\n",
        "print(f\"Model moved to device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 10/10 [00:00<00:00, 2475.83 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created fine-tuning dataset with 10 examples\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "domain_data = [\n",
        "    \"Question: What is machine learning? Answer: Machine learning is a method of data analysis that automates analytical model building.\",\n",
        "    \"Question: What is deep learning? Answer: Deep learning is a subset of machine learning that uses neural networks with multiple layers.\",\n",
        "    \"Question: What is NLP? Answer: Natural Language Processing is a field of AI that focuses on understanding and generating human language.\",\n",
        "    \"Question: What are transformers? Answer: Transformers are neural network architectures that use attention mechanisms to process sequences.\",\n",
        "    \"Question: What is BERT? Answer: BERT is a bidirectional transformer model that reads text in both directions simultaneously.\",\n",
        "    \"Question: What is GPT? Answer: GPT is a generative pre-trained transformer model that generates text autoregressively.\",\n",
        "    \"Question: What is fine-tuning? Answer: Fine-tuning is the process of adapting a pre-trained model to a specific task or domain.\",\n",
        "    \"Question: What is LoRA? Answer: LoRA is a parameter-efficient fine-tuning technique that uses low-rank matrices.\",\n",
        "    \"Question: What is RAG? Answer: Retrieval Augmented Generation combines information retrieval with text generation.\",\n",
        "    \"Question: What is a vector database? Answer: A vector database stores embeddings of documents for semantic search.\"\n",
        "]\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return fine_tune_tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": domain_data})\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "print(f\"Created fine-tuning dataset with {len(dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n",
            "LoRA configuration applied to model\n",
            "PEFT model on device: mps\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"]\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(fine_tune_model, lora_config)\n",
        "peft_model = peft_model.to(device)\n",
        "peft_model.print_trainable_parameters()\n",
        "\n",
        "print(\"LoRA configuration applied to model\")\n",
        "print(f\"PEFT model on device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting fine-tuning with LoRA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.834300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning completed\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_model\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=10,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=fine_tune_tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"Starting fine-tuning with LoRA...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Prompt: Question: What is machine learning? Answer:\n",
            "Generated Response: Question: What is machine learning? Answer: Machine learning is the process of learning.\n",
            "\n",
            "What is machine learning?\n",
            "\n",
            "Machine learning is a process of learning.\n",
            "\n",
            "Machine learning is a process of.\n",
            "\n",
            "Machine learning is a process\n"
          ]
        }
      ],
      "source": [
        "test_prompt = \"Question: What is machine learning? Answer:\"\n",
        "inputs = fine_tune_tokenizer(test_prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = peft_model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
        "\n",
        "generated_text = fine_tune_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Test Prompt:\", test_prompt)\n",
        "print(\"Generated Response:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part C: Model Context Protocol (MCP) Server\n",
        "\n",
        "This section implements an MCP Server that bridges an AI client to external data sources. The server provides two tools: a capital city lookup tool and a population lookup tool for various countries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MCP Server class and helper functions defined\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import socket\n",
        "from typing import Dict, Any\n",
        "from http.server import HTTPServer, BaseHTTPRequestHandler\n",
        "\n",
        "class MCPServer(BaseHTTPRequestHandler):\n",
        "    def do_POST(self):\n",
        "        content_length = int(self.headers['Content-Length'])\n",
        "        post_data = self.rfile.read(content_length)\n",
        "        request_data = json.loads(post_data.decode('utf-8'))\n",
        "        \n",
        "        if request_data.get('method') == 'tools/list':\n",
        "            response = {\n",
        "                \"tools\": [\n",
        "                    {\n",
        "                        \"name\": \"get_capital\",\n",
        "                        \"description\": \"Get the capital city of a country\",\n",
        "                        \"inputSchema\": {\n",
        "                            \"type\": \"object\",\n",
        "                            \"properties\": {\n",
        "                                \"country\": {\n",
        "                                    \"type\": \"string\",\n",
        "                                    \"description\": \"Name of the country\"\n",
        "                                }\n",
        "                            },\n",
        "                            \"required\": [\"country\"]\n",
        "                        }\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": \"get_population\",\n",
        "                        \"description\": \"Get the population of a country\",\n",
        "                        \"inputSchema\": {\n",
        "                            \"type\": \"object\",\n",
        "                            \"properties\": {\n",
        "                                \"country\": {\n",
        "                                    \"type\": \"string\",\n",
        "                                    \"description\": \"Name of the country\"\n",
        "                                }\n",
        "                            },\n",
        "                            \"required\": [\"country\"]\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        elif request_data.get('method') == 'tools/call':\n",
        "            tool_name = request_data.get('params', {}).get('name')\n",
        "            arguments = request_data.get('params', {}).get('arguments', {})\n",
        "            \n",
        "            if tool_name == \"get_capital\":\n",
        "                result = self.get_capital(arguments.get('country', 'United States'))\n",
        "            elif tool_name == \"get_population\":\n",
        "                result = self.get_population(arguments.get('country', 'United States'))\n",
        "            else:\n",
        "                result = {\"error\": \"Unknown tool\"}\n",
        "            \n",
        "            response = {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result)}]}\n",
        "        else:\n",
        "            response = {\"error\": \"Unknown method\"}\n",
        "        \n",
        "        self.send_response(200)\n",
        "        self.send_header('Content-type', 'application/json')\n",
        "        self.end_headers()\n",
        "        self.wfile.write(json.dumps(response).encode('utf-8'))\n",
        "    \n",
        "    def get_capital(self, country: str) -> Dict[str, Any]:\n",
        "        capitals = {\n",
        "            \"United States\": \"Washington, D.C.\",\n",
        "            \"USA\": \"Washington, D.C.\",\n",
        "            \"Ukraine\": \"Kyiv\",\n",
        "            \"France\": \"Paris\",\n",
        "            \"Germany\": \"Berlin\",\n",
        "            \"United Kingdom\": \"London\",\n",
        "            \"UK\": \"London\",\n",
        "            \"Japan\": \"Tokyo\",\n",
        "            \"China\": \"Beijing\",\n",
        "            \"India\": \"New Delhi\",\n",
        "            \"Canada\": \"Ottawa\",\n",
        "            \"Australia\": \"Canberra\",\n",
        "            \"Brazil\": \"Brasília\",\n",
        "            \"Russia\": \"Moscow\",\n",
        "            \"Italy\": \"Rome\",\n",
        "            \"Spain\": \"Madrid\",\n",
        "            \"Mexico\": \"Mexico City\",\n",
        "            \"South Korea\": \"Seoul\",\n",
        "            \"Argentina\": \"Buenos Aires\",\n",
        "            \"South Africa\": \"Cape Town\"\n",
        "        }\n",
        "        \n",
        "        country_key = country.title()\n",
        "        if country_key in capitals:\n",
        "            return {\n",
        "                \"country\": country,\n",
        "                \"capital\": capitals[country_key]\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"country\": country,\n",
        "                \"capital\": \"Unknown\",\n",
        "                \"note\": f\"Capital not found in database for {country}\"\n",
        "            }\n",
        "    \n",
        "    def get_population(self, country: str) -> Dict[str, Any]:\n",
        "        populations = {\n",
        "            \"United States\": 331900000,\n",
        "            \"USA\": 331900000,\n",
        "            \"Ukraine\": 43790000,\n",
        "            \"France\": 67800000,\n",
        "            \"Germany\": 83200000,\n",
        "            \"United Kingdom\": 67000000,\n",
        "            \"UK\": 67000000,\n",
        "            \"Japan\": 125800000,\n",
        "            \"China\": 1402000000,\n",
        "            \"India\": 1380000000,\n",
        "            \"Canada\": 38200000,\n",
        "            \"Australia\": 25700000,\n",
        "            \"Brazil\": 215300000,\n",
        "            \"Russia\": 146200000,\n",
        "            \"Italy\": 59000000,\n",
        "            \"Spain\": 47400000,\n",
        "            \"Mexico\": 128900000,\n",
        "            \"South Korea\": 51700000,\n",
        "            \"Argentina\": 45800000,\n",
        "            \"South Africa\": 60000000\n",
        "        }\n",
        "        \n",
        "        country_key = country.title()\n",
        "        if country_key in populations:\n",
        "            pop = populations[country_key]\n",
        "            return {\n",
        "                \"country\": country,\n",
        "                \"population\": pop,\n",
        "                \"population_formatted\": f\"{pop:,}\"\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"country\": country,\n",
        "                \"population\": \"Unknown\",\n",
        "                \"note\": f\"Population data not found in database for {country}\"\n",
        "            }\n",
        "    \n",
        "    def log_message(self, format, *args):\n",
        "        pass\n",
        "\n",
        "def find_free_port(start_port=8000, max_attempts=10):\n",
        "    for port in range(start_port, start_port + max_attempts):\n",
        "        try:\n",
        "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "                s.bind(('localhost', port))\n",
        "                return port\n",
        "        except OSError:\n",
        "            continue\n",
        "    raise RuntimeError(f\"Could not find free port in range {start_port}-{start_port + max_attempts}\")\n",
        "\n",
        "print(\"MCP Server class and helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MCP Server started on http://localhost:8000\n",
            "Server is ready to accept requests\n"
          ]
        }
      ],
      "source": [
        "port = find_free_port(8000)\n",
        "server = HTTPServer(('localhost', port), MCPServer)\n",
        "\n",
        "import threading\n",
        "server_thread = threading.Thread(target=server.serve_forever, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "print(f\"MCP Server started on http://localhost:{port}\")\n",
        "print(\"Server is ready to accept requests\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MCP Server Testing\n",
        "\n",
        "The server can be tested using direct API calls or integrated with an LLM client like Google Gemini. Below are demonstrations of the server functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TESTING MCP SERVER\n",
            "======================================================================\n",
            "\n",
            "1. Listing available tools:\n",
            "   - get_capital: Get the capital city of a country\n",
            "   - get_population: Get the population of a country\n",
            "\n",
            "2. Testing get_capital tool:\n",
            "   Country: Ukraine\n",
            "   Capital: Kyiv\n",
            "\n",
            "3. Testing get_population tool:\n",
            "   Country: United States\n",
            "   Population: 331,900,000\n",
            "\n",
            "======================================================================\n",
            "MCP SERVER TESTING COMPLETE\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "base_url = f\"http://localhost:{port}\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TESTING MCP SERVER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. Listing available tools:\")\n",
        "response = requests.post(base_url, json={'method': 'tools/list'}, timeout=5)\n",
        "tools = response.json().get('tools', [])\n",
        "for tool in tools:\n",
        "    print(f\"   - {tool['name']}: {tool['description']}\")\n",
        "\n",
        "print(\"\\n2. Testing get_capital tool:\")\n",
        "response = requests.post(base_url, json={\n",
        "    'method': 'tools/call',\n",
        "    'params': {\n",
        "        'name': 'get_capital',\n",
        "        'arguments': {'country': 'Ukraine'}\n",
        "    }\n",
        "}, timeout=5)\n",
        "capital_result = json.loads(response.json()['content'][0]['text'])\n",
        "print(f\"   Country: {capital_result.get('country')}\")\n",
        "print(f\"   Capital: {capital_result.get('capital')}\")\n",
        "\n",
        "print(\"\\n3. Testing get_population tool:\")\n",
        "response = requests.post(base_url, json={\n",
        "    'method': 'tools/call',\n",
        "    'params': {\n",
        "        'name': 'get_population',\n",
        "        'arguments': {'country': 'United States'}\n",
        "    }\n",
        "}, timeout=5)\n",
        "pop_result = json.loads(response.json()['content'][0]['text'])\n",
        "print(f\"   Country: {pop_result.get('country')}\")\n",
        "print(f\"   Population: {pop_result.get('population_formatted')}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MCP SERVER TESTING COMPLETE\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing with Google Gemini\n",
        "\n",
        "The MCP server can be integrated with Google Gemini to demonstrate how an LLM can use the custom tools to answer questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Query: What's the capital of Ukraine?\n",
            "======================================================================\n",
            "\n",
            "[Calling MCP tool: get_capital with country='Ukraine']\n",
            "Response:\n",
            "The capital of Ukraine is Kyiv.\n",
            "\n",
            "[Tool Result: {\n",
            "  \"country\": \"Ukraine\",\n",
            "  \"capital\": \"Kyiv\"\n",
            "}]\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Query: What's the population of the United States?\n",
            "======================================================================\n",
            "\n",
            "[Calling MCP tool: get_population with country='United States']\n",
            "Response:\n",
            "The population of the United States is 331,900,000.\n",
            "\n",
            "[Tool Result: {\n",
            "  \"country\": \"United States\",\n",
            "  \"population\": 331900000,\n",
            "  \"population_formatted\": \"331,900,000\"\n",
            "}]\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Query: Tell me the capital of France and the population of Japan\n",
            "======================================================================\n",
            "\n",
            "[Calling MCP tool: get_capital with country='France']\n",
            "Response:\n",
            "The capital of France is Paris.\n",
            "\n",
            "I don't have information about the population of Japan in my current results.\n",
            "\n",
            "[Tool Result: {\n",
            "  \"country\": \"France\",\n",
            "  \"capital\": \"Paris\"\n",
            "}]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\", \"AIzaSyAxi7IUunFMRMIsiZnRZ9KKe-TYIJUJ7ZQ\"))\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "MCP_SERVER_URL = f\"http://localhost:{port}\"\n",
        "\n",
        "def call_mcp_tool(tool_name, arguments):\n",
        "    try:\n",
        "        response = requests.post(MCP_SERVER_URL, json={\n",
        "            'method': 'tools/call',\n",
        "            'params': {\n",
        "                'name': tool_name,\n",
        "                'arguments': arguments\n",
        "            }\n",
        "        }, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            result = json.loads(response.json()['content'][0]['text'])\n",
        "            return result\n",
        "        else:\n",
        "            return {\"error\": f\"Server returned status {response.status_code}\"}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "def chat_with_gemini_and_tools(user_query):\n",
        "    tool_called = False\n",
        "    tool_result = None\n",
        "    query_lower = user_query.lower()\n",
        "    \n",
        "    if \"capital\" in query_lower:\n",
        "        country = \"United States\"\n",
        "        countries = {\n",
        "            \"ukraine\": \"Ukraine\",\n",
        "            \"france\": \"France\",\n",
        "            \"germany\": \"Germany\",\n",
        "            \"united kingdom\": \"United Kingdom\",\n",
        "            \"uk\": \"United Kingdom\",\n",
        "            \"japan\": \"Japan\",\n",
        "            \"china\": \"China\",\n",
        "            \"india\": \"India\",\n",
        "            \"canada\": \"Canada\",\n",
        "            \"australia\": \"Australia\",\n",
        "            \"brazil\": \"Brazil\",\n",
        "            \"russia\": \"Russia\",\n",
        "            \"italy\": \"Italy\",\n",
        "            \"spain\": \"Spain\",\n",
        "            \"mexico\": \"Mexico\",\n",
        "            \"south korea\": \"South Korea\",\n",
        "            \"argentina\": \"Argentina\",\n",
        "            \"south africa\": \"South Africa\"\n",
        "        }\n",
        "        \n",
        "        for key, val in countries.items():\n",
        "            if key in query_lower:\n",
        "                country = val\n",
        "                break\n",
        "        \n",
        "        if \"united states\" in query_lower or \"usa\" in query_lower or \"us\" in query_lower:\n",
        "            country = \"United States\"\n",
        "        \n",
        "        print(f\"\\n[Calling MCP tool: get_capital with country='{country}']\")\n",
        "        tool_result = call_mcp_tool(\"get_capital\", {\"country\": country})\n",
        "        tool_called = True\n",
        "        \n",
        "    elif \"population\" in query_lower or \"people\" in query_lower or \"inhabitants\" in query_lower:\n",
        "        country = \"United States\"\n",
        "        countries = {\n",
        "            \"ukraine\": \"Ukraine\",\n",
        "            \"france\": \"France\",\n",
        "            \"germany\": \"Germany\",\n",
        "            \"united kingdom\": \"United Kingdom\",\n",
        "            \"uk\": \"United Kingdom\",\n",
        "            \"japan\": \"Japan\",\n",
        "            \"china\": \"China\",\n",
        "            \"india\": \"India\",\n",
        "            \"canada\": \"Canada\",\n",
        "            \"australia\": \"Australia\",\n",
        "            \"brazil\": \"Brazil\",\n",
        "            \"russia\": \"Russia\",\n",
        "            \"italy\": \"Italy\",\n",
        "            \"spain\": \"Spain\",\n",
        "            \"mexico\": \"Mexico\",\n",
        "            \"south korea\": \"South Korea\",\n",
        "            \"argentina\": \"Argentina\",\n",
        "            \"south africa\": \"South Africa\"\n",
        "        }\n",
        "        \n",
        "        for key, val in countries.items():\n",
        "            if key in query_lower:\n",
        "                country = val\n",
        "                break\n",
        "        \n",
        "        if \"united states\" in query_lower or \"usa\" in query_lower or \"us\" in query_lower:\n",
        "            country = \"United States\"\n",
        "        \n",
        "        print(f\"\\n[Calling MCP tool: get_population with country='{country}']\")\n",
        "        tool_result = call_mcp_tool(\"get_population\", {\"country\": country})\n",
        "        tool_called = True\n",
        "    \n",
        "    if tool_called and tool_result:\n",
        "        result_prompt = f\"\"\"Based on the tool result, provide a helpful answer to the user.\n",
        "\n",
        "Tool Result: {json.dumps(tool_result, indent=2)}\n",
        "\n",
        "User question: {user_query}\n",
        "\n",
        "Provide a natural, conversational answer using the tool result:\"\"\"\n",
        "        \n",
        "        try:\n",
        "            final_response = model.generate_content(result_prompt)\n",
        "            if final_response.candidates and len(final_response.candidates) > 0:\n",
        "                candidate = final_response.candidates[0]\n",
        "                if candidate.content and candidate.content.parts:\n",
        "                    response_text = candidate.content.parts[0].text\n",
        "                else:\n",
        "                    response_text = \"Tool result received. Here's the data:\\n\" + json.dumps(tool_result, indent=2)\n",
        "            else:\n",
        "                response_text = \"Tool result received. Here's the data:\\n\" + json.dumps(tool_result, indent=2)\n",
        "        except Exception as e:\n",
        "            response_text = f\"Tool executed successfully. Result: {json.dumps(tool_result, indent=2)}\\n\\n(Error formatting response: {str(e)})\"\n",
        "        \n",
        "        return f\"{response_text}\\n\\n[Tool Result: {json.dumps(tool_result, indent=2)}]\"\n",
        "    \n",
        "    return \"No relevant tool found for this query.\"\n",
        "\n",
        "test_queries = [\n",
        "    \"What's the capital of Ukraine?\",\n",
        "    \"What's the population of the United States?\",\n",
        "    \"Tell me the capital of France and the population of Japan\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"=\"*70)\n",
        "    try:\n",
        "        result = chat_with_gemini_and_tools(query)\n",
        "        print(f\"Response:\\n{result}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resources\n",
        "\n",
        "1. **Hugging Face Transformers:** https://huggingface.co/docs/transformers/\n",
        "2. **LangChain Documentation:** https://python.langchain.com/\n",
        "3. **PEFT (Parameter-Efficient Fine-Tuning):** https://huggingface.co/docs/peft/\n",
        "4. **Model Context Protocol:** https://modelcontextprotocol.io/\n",
        "5. **FAISS Vector Database:** https://github.com/facebookresearch/faiss\n",
        "6. **Google Gemini API:** https://ai.google.dev/\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
